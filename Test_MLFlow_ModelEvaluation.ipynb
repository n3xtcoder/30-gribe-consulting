{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhMENa1XCbrICmtFO2tdCj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ML Workflow Instructions for Generative Chat Assistant\n",
        "\n",
        "This notebook aims at providing an overall guideline on how to set up a workflow using MLFlow to handle experiment tracking, model management, and prompt evaluation over a custom GPT model.\n",
        "\n",
        "The key aspects taken into consideration are:\n",
        "\n",
        "\n",
        "\n",
        "*   Parameters log: learning rate, number of layers in a neural network, batch size, data augmentation techniques\n",
        "*   Evaluation metrics: accuracy, precision and recall, F1 score, loss value, execution time\n",
        "*   Artifacts: trained model weights, model architecture diagrams, training logs, evaluation reports, visualizations (e.g., confusion matrices, ROC curves)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wHEShs2ryW6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing dependencies (MLFlow)"
      ],
      "metadata": {
        "id": "_357FWYYzCi1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEuQwBVqyWIC"
      },
      "outputs": [],
      "source": [
        "!pip install mlflow transformers torch bert_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selection of evaluation metrics\n",
        "\n",
        "Some metrics that are important for evaluating the generated text include:\n",
        "\n",
        "*   Exact Match: Measures if the generated text matches the ground truth exactly.\n",
        "*   Token Overlap: Measures the overlap between tokens in the generated text and the ground truth.\n",
        "*   BLEU Score: Measures the similarity between the generated text and the ground truth using n-gram overlaps.\n",
        "*   ROUGE Score: Measures recall-oriented metrics for evaluating text generation tasks.\n",
        "*  BERTScore: Uses pre-trained BERT embeddings to measure the similarity of the generated text to the reference text. Captures semantic meaning better than surface-level n-gram overlap.\n",
        "\n",
        "Keeping this in mind, then we implement the logic."
      ],
      "metadata": {
        "id": "4ssA87P4zkmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Initialize your custom GPT model\n",
        "gpt_model = OpenAI(api_key=\"your_api_key\")\n",
        "\n",
        "# Define the prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"Your custom prompt template here\",\n",
        "    input_variables=[\"variable1\", \"variable2\"]\n",
        ")\n",
        "\n",
        "# Ground truth data\n",
        "ground_truths = {\n",
        "    \"prompt1\": \"expected_response1\",\n",
        "    \"prompt2\": \"expected_response2\",\n",
        "    # Add more ground truth data\n",
        "}\n",
        "\n",
        "def evaluate_response(prompt, response, ground_truth):\n",
        "    # Exact Match\n",
        "    exact_match = response == ground_truth\n",
        "\n",
        "    # Token Overlap\n",
        "    response_tokens = set(response.split())\n",
        "    ground_truth_tokens = set(ground_truth.split())\n",
        "    token_overlap = len(response_tokens & ground_truth_tokens) / len(ground_truth_tokens)\n",
        "\n",
        "    # BLEU Score\n",
        "    bleu_score = sentence_bleu([ground_truth.split()], response.split())\n",
        "\n",
        "    # ROUGE Score\n",
        "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = rouge.score(ground_truth, response)\n",
        "\n",
        "    # Log metrics to MLFlow\n",
        "    mlflow.log_param(\"prompt\", prompt)\n",
        "    mlflow.log_metric(\"exact_match\", exact_match)\n",
        "    mlflow.log_metric(\"token_overlap\", token_overlap)\n",
        "    mlflow.log_metric(\"bleu_score\", bleu_score)\n",
        "    mlflow.log_metric(\"rouge1\", rouge_scores['rouge1'].fmeasure)\n",
        "    mlflow.log_metric(\"rougeL\", rouge_scores['rougeL'].fmeasure)\n",
        "\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print(f\"Ground Truth: {ground_truth}\")\n",
        "    print(f\"Exact Match: {exact_match}\")\n",
        "    print(f\"Token Overlap: {token_overlap}\")\n",
        "    print(f\"BLEU Score: {bleu_score}\")\n",
        "    print(f\"ROUGE-1 F1 Score: {rouge_scores['rouge1'].fmeasure}\")\n",
        "    print(f\"ROUGE-L F1 Score: {rouge_scores['rougeL'].fmeasure}\")\n",
        "\n",
        "# Start MLFlow run\n",
        "mlflow.start_run(run_name=\"prompt_evaluation_with_ground_truth\")\n",
        "\n",
        "# Generate and evaluate responses\n",
        "for prompt, ground_truth in ground_truths.items():\n",
        "    generated_prompt = prompt_template.generate({\"variable1\": prompt, \"variable2\": \"static_value\"})\n",
        "    response = gpt_model(generated_prompt)\n",
        "    evaluate_response(generated_prompt, response, ground_truth)\n",
        "\n",
        "# End MLFlow run\n",
        "mlflow.end_run()\n"
      ],
      "metadata": {
        "id": "ETSWcmmUzkBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Thresholds or Criteria\n",
        "\n",
        "To determine if the generated text is aligned with the ground truth, you need to define thresholds for the metrics. These thresholds will depend on your specific use case and requirements.\n",
        "\n",
        "Example Thresholds:\n",
        "\n",
        "*   Exact Match: You might require an exact match for critical applications.\n",
        "*   Token Overlap: You might set a threshold of 0.7, meaning 70% of the tokens should overlap with the ground truth.\n",
        "*   BLEU Score: A BLEU score above 0.5 might be considered acceptable.\n",
        "*   ROUGE Scores: ROUGE-1 and ROUGE-L scores above 0.5 might be considered acceptable."
      ],
      "metadata": {
        "id": "kPnvFvJb3dUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpret Results"
      ],
      "metadata": {
        "id": "mcsrcZSU3r2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "client = MlflowClient()\n",
        "run_id = \"your_run_id\"  # Replace with your run ID\n",
        "\n",
        "# Retrieve logged metrics\n",
        "metrics = client.get_run(run_id).data.metrics\n",
        "\n",
        "# Check against thresholds\n",
        "exact_match = metrics.get('exact_match', 0) == 1\n",
        "token_overlap = metrics.get('token_overlap', 0) >= 0.7\n",
        "bleu_score = metrics.get('bleu_score', 0) >= 0.5\n",
        "rouge1 = metrics.get('rouge1', 0) >= 0.5\n",
        "rougeL = metrics.get('rougeL', 0) >= 0.5\n",
        "\n",
        "# Determine if the response is aligned with the ground truth\n",
        "is_aligned = exact_match or (token_overlap and bleu_score and rouge1 and rougeL)\n",
        "\n",
        "if is_aligned:\n",
        "    print(\"The generated text is aligned with the ground truth.\")\n",
        "else:\n",
        "    print(\"The generated text is not aligned with the ground truth.\")\n"
      ],
      "metadata": {
        "id": "Anun_cN13v2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating using the BERTScore\n",
        "\n",
        "Here with the BERTScore we compute precision, recall, and F1 score based on BERT embeddings, providing a semantic similarity measure between the generated text and the ground truth. BERTScore provides an evaluation method more aligned with human judgment - which is pretty much more suitable for current LLMs."
      ],
      "metadata": {
        "id": "zUqAkhm_NS6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import mlflow\n",
        "from bert_score import score\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = \"your_openai_api_key\"\n",
        "\n",
        "# Ground truth data\n",
        "ground_truths = {\n",
        "    \"prompt1\": \"expected_response1\",\n",
        "    \"prompt2\": \"expected_response2\",\n",
        "    # Add more ground truth data\n",
        "}\n",
        "\n",
        "def generate_text(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=50\n",
        "    )\n",
        "    return response['choices'][0]['message']['content']\n",
        "\n",
        "def evaluate_response(prompt, response, ground_truth):\n",
        "    # BERTScore calculation\n",
        "    P, R, F1 = score([response], [ground_truth], lang=\"en\", verbose=True)\n",
        "\n",
        "    # Convert tensor values to float for logging\n",
        "    precision = P.mean().item()\n",
        "    recall = R.mean().item()\n",
        "    f1_score = F1.mean().item()\n",
        "\n",
        "    # Log metrics to MLFlow\n",
        "    mlflow.log_param(\"prompt\", prompt)\n",
        "    mlflow.log_metric(\"bert_precision\", precision)\n",
        "    mlflow.log_metric(\"bert_recall\", recall)\n",
        "    mlflow.log_metric(\"bert_f1\", f1_score)\n",
        "\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print(f\"Ground Truth: {ground_truth}\")\n",
        "    print(f\"BERT Precision: {precision}\")\n",
        "    print(f\"BERT Recall: {recall}\")\n",
        "    print(f\"BERT F1 Score: {f1_score}\")\n",
        "\n",
        "# Start MLFlow run\n",
        "mlflow.start_run(run_name=\"prompt_evaluation_with_bertscore_and_gpt3.5\")\n",
        "\n",
        "# Generate and evaluate responses\n",
        "for prompt, ground_truth in ground_truths.items():\n",
        "    response = generate_text(prompt, model=\"gpt-3.5-turbo\")\n",
        "    evaluate_response(prompt, response, ground_truth)\n",
        "\n",
        "# End MLFlow run\n",
        "mlflow.end_run()\n"
      ],
      "metadata": {
        "id": "POKJ2c5CNbsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Useful resources:\n",
        "\n",
        "*   [The complete guide to string similarity algorithms](https://yassineelkhal.medium.com/the-complete-guide-to-string-similarity-algorithms-1290ad07c6b7) - useful to understand the diverse metrics available and which one may be the most relevants for standard scenarios of specific text content on particular prompts.\n",
        "*   [Similarity Coefficients: A Beginner’s Guide to Measuring String Similarity](https://medium.com/@igniobydigitate/similarity-coefficients-a-beginners-guide-to-measuring-string-similarity-d84da77e8c5a) - explanation at a higher level of some metrics to evaluate text similarity.\n",
        "*   [About BERTScore](https://medium.com/@abonia/bertscore-explained-in-5-minutes-0b98553bfb71).\n",
        "*   [BERTScore usage and interactive testing](https://huggingface.co/spaces/evaluate-metric/bertscore)."
      ],
      "metadata": {
        "id": "3b6cZSXoD-nc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Insights on Chat Assistants vs Chat Completions\n",
        "\n",
        "When using OpenAI's GPT models allows for two main ways of handling conversations with users.\n",
        "\n",
        "On one hand, [Chat Assistants](https://platform.openai.com/docs/assistants/overview), whose one of the key features is about persistent Threads (conversation sessions), which means it enables keeping history of the conversation. Another point, it handles more complex data formats in the messages between the user and system: they can include text, images, and other files.\n",
        "\n",
        "On the other hand, [Chat Completions](https://platform.openai.com/docs/guides/chat-completions). These don't count on persistent conversation's history; instead, the messages that want to be kept in the ongoing conversation have to be manually added as part of the \"context\". Also Chat Completions support text and image, but only as an input, and can output text content (e.g., code and JSON).\n",
        "\n",
        "**Recommendation:** Check the most suitable also taking into account pricing and token usage. Make tests with real users to understand how long the chat sessions tend to be and whether it is really convenient to store a history (e.g., new questions on the same session related to a very early message). Perhaps if questions are more direct and instructive on specific pieces of content, including just a few messages as context is enough - so Chat Completions should be fine."
      ],
      "metadata": {
        "id": "mohO6ECkZt_S"
      }
    }
  ]
}